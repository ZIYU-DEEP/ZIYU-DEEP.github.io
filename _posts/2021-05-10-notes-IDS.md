# An Information-Theoretic View of TS
[TOC]

*This note is based on the [talk](https://vod.video.cornell.edu/media/ORIE+Colloquium%2C++2015-01-16+-+Daniel+RussoA+Learning+to+Optimize/1_cmc6q21l/102313041) of Daniel Russo, and his [2014](https://arxiv.org/abs/1403.5556) and [2016](https://arxiv.org/abs/1403.5341) paper with Van Roy.*


## 1. Overview
My perspective:
- In the scenarios when actions are informationally-entangled, we are able to make use of the information structure, and act more efficiently than TS and UCB.
    - i.e. Sampling one action provides information about other actions.
    - Q: How is this related to contexual bandits?

Author's perspective:
- Under TS, large regret --> large information gain about the optimum.
    - Since it is acquiring a lot of information about which action is optimal.
<br>

## 2. Case Study on Troubling Examples for TS
### 2.1 Example: Assortment Optimization
#### Problem Setting
- Learn through *repeated interactions* with a *single customer* of unknown type
    - N customer types
    - Many products, each may be suitable to a certain type
    - Action: offer M products
    - Customer chooses one product per period

#### TS Solution
- TS offer an assortment geared for a **single type**.
- At each time, guess the type of the customer and only offer that specific type of products
    - Learn something, but not so much.

#### Better Solution
- If we want to learn, we need to **diversify**.
- IDS offers a diverse **assortment**.
    - Reduces regret by a factor of M.

### 2.2. Example: A 1-sparse linear bandit
#### Problem Setting
- Linear model: $R_{t}=\theta^{T} A_{t}$.
- Action set: $\mathcal{A}=\left\{\frac{x}{\|x\|_{1}}: x \in\{0,1\}^{d}, x \neq 0\right\}$.
    - i.e., normalized binary action set
- $\theta$ drawn uniformly at random from $\Theta=\left\{\theta \in\{0,1\}^{d}:\|\theta\|_{0}=1\right\}$.
    - i.e., only one 1 appears in the parameter vector, and its position is unknown.

#### TS Solution
- Sample: $\hat{\theta}=(0, \ldots .0, \underbrace{1}_{i}, 0 \ldots, 0)$
- Play: $\hat{a}=(0, \ldots, \underbrace{1}_{i}, 0 \ldots, 0)$
    - i.e. has the highest inner product with the parameter vector
    - Sparse sample leads to sparse action.
- Observe: $\theta^{T} \hat{a}=1\left\{\theta_{i}=1\right\}$
    - i.e. the reward is always 0 unless the guessed position $i$ is correct
    - Most cases time get it wrong
- TS and UCB requires $\Omega(d)$ samples to identify the optimum.

#### Better Solution
- IDS requires $log_{2}(d)$ samples.
    - A simple heuristic: set half of the components of the actions to be 1, and half to be 0.
    - Then learn which half contains the true one.
    - Then reduce the size of actions.
    - i.e. Think how much information you can acquire when you take a different information.

### 2.3. Key Takeaways on TS and UCB
- They often offers extremely *simple* and provably *statistically efficient* solutions.
- But they could be inefficient for some natural problem:
    - As they do not carefully **quantify the information** provided by actions (about the optimal action, and other actions not taken).
- We can try to model and exploit the **information structure** of problems.
<br>

## 3. Measure of Information
- **Optimal action**: $A^{*} \in \arg \max _{a \in \mathcal{A}} f_{\theta}(a)$
    - It can be considered as a function of $\theta$ and thus becomes an random variable.
- **Entropy**: $H_{t}(A^{*})$
    - Given all the data we have seen at time $t$, it measures how uncertain we are about which action is optimal.
    - To make this uncertainty smaller, we need mutual information.
- **Mutual information** between $A^{*}$ and $\underbrace{\left(A_{t}, Y_{t}\right)}_{\text {next observation}}$.
    - Measures expected reduction in entropy:
    $$
    I_{t}\left(A^{*} ;\left(A_{t}, Y_{t}\right)\right)=\mathbb{E}_{t}\left[H_{t}\left(A^{*}\right)-H_{t+1}\left(A^{*}\right)\right]
    $$
    - i.e. Given all the data I have so far, how much I expect my next observation to reduce my uncertainty about which action is the optimal.
    - This only quantifies information relevant to **optimal actions**, which is different from acquiring information about the **true $\theta$**.
<br>

## 4. Information Directed Sampling (IDS)
$$
\psi_{t}=\frac{\mathbb{E}_{t}\left[f_{\theta}\left(A^{*}\right)-f_{\theta}\left(A_{t}\right)\right]^{2}}{I_{t}\left(A^{*} ;\left(A_{t}, Y_{t}\right)\right)}=\frac{(\text { expected regret })^{2}}{\text { Mutual information }}
$$
- The "cost" per bit of information.
- IDS selects the **action distribution** that minimizes $\psi_{t}$.
- Proxy to an intractable multi-period problem.
    - At each time step, solves a single-period optimization problems, instead of looking ahead of the whole time horizon.
    - Balance between getting high reward and learning.
- This way quantifies the information provided by actions and make use of it.
<br>

## 5. Theoretical Guarantees
### 5.1. Linear Case
Suppose IDS is applied to a $d$ dimensional linear bandit problem that $f_{\theta}(a)=a^{T} \theta$:
$$
\mathbb{E}[\operatorname{Regret}(T)] \leq \sqrt{\frac{\operatorname{Entropy}\left(A^{*}\right) d T}{2}}
$$
- Dimension of the linear model $d$:
    - Captures *the complexity of the problem's "information structure"*.
    - i.e., how hard it is to acquire new information.
- Entropy of the optimal action distribution $\operatorname{Entropy}$:
    - Captures *the magnitude of the agent's initial uncertainty*.
    - i.e., how much to learn about the optimum.

### 5.2. General Case
For any algorithm:
$$
\mathbb{E}[\operatorname{Regret}(T)] \leq \sqrt{\operatorname{Entropy}\left(A^{*}\right) \bar{\Psi}_{T} T}
$$
$$
\bar{\Psi}_{T}=\frac{1}{T} \sum_{t=1}^{T} \mathbb{E}\left[\Psi_{t}\right]
$$
- We get tight bound in certain cases:
    - $\Psi_{t} \leq |\mathcal{A}| / 2$ always
    - $\Psi_{t} \leq d / 2$ for linear optimization under bandit feedback
    - $\Psi_{t} \leq 1 / 2$ for full information problem
<br>

## 6. Additional Thoughts
- The key idea is that when actions are informationally relavant, (and when the problem is time-sensitive), IDS may perform better.

- [Conceptual] What is difference between getting information about the optimum, and getting information about $\theta$?
    - I feels it is something like opportunity cost v.s. actual cost.
- [Conceptual] What is the problem (e.g. computational complexity) of IDS? How to improve it?
    - Look into the follow-up work on rate-distortion approaches ([NeurIPS '18](https://arxiv.org/pdf/1805.11845.pdf), [Preprint '21](https://arxiv.org/abs/2101.06197)), hypermodels for exploration ([ICLR '20](https://openreview.net/pdf?id=ryx6WgStPB)).
    - Is there a way to model the information relavance of actions, in order to reduce inefficient exploration?
- [Conceptual] Can we decouple the increment of mutual information and the reduction of regret?
    - An ICLR '20 rejected paper: [Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices](https://openreview.net/forum?id=rSwTMomgCz).

$$
\mathbb{E}\left[f_{\theta}\left(A^{*}\right)-f_{\theta}\left(A_{t}\right] \leq \mathbb{E}\left[U_{t}\left(A_{t}\right)-f_{\theta}\left(A_{t}\right)\right]+\mathbb{P}\left\{U_{t}\left(A^{*}\right)<f_{\theta}\left(A^{*}\right)\right\}\right.
$$
